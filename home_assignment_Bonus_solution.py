# -*- coding: utf-8 -*-
"""home_assignment_Bonus_solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18coIC8Ts4FzElq7L1JcePZHhOOU5H1KP

Bonus Assignment

Student Name: SAI SRAVAN CHINTALA

Student ID: 700773836

Question 1: Question Answering with Transformers

Use Hugging Face’s transformers library to build a simple question answering system using pre-trained models.
Setup Instructions:
Before starting, make sure your Python environment has the transformers and torch libraries installed.

Assignment Tasks:

1. Basic Pipeline Setup
•	Import the pipeline function from transformers.
•	Initialize a question-answering pipeline using the default model.
•	Ask a question based on the given context.
Expected output
•	'answer': 'Charles Babbage' (or close variant)
•	A confidence 'score' key with a float value above 0.65
•	Valid 'start' and 'end' indices

2. Use a Custom Pretrained Model
•	Switch to a different QA model like deepset/roberta-base-squad2.
Expected output
•	'answer': 'Charles Babbage'
•	'score' greater than 0.70
•	Include 'start' and 'end' indices

3. Test on Your Own Example
•	Write your own 2–3 sentence context.
•	Ask two different questions from it and print the answers.
Expected output
•	Include a relevant, meaningful 'answer' to each question
•	Display a 'score' above 0.70 for each answer
"""

from transformers import pipeline

# -----------------------------------------------------------
# Task 1: Basic Pipeline Setup using default pretrained model
# -----------------------------------------------------------

# Step 1: Initialize the question answering pipeline
qa_pipeline = pipeline("question-answering")

# Step 2: Define context and question
context = (
    "Charles Babbage, an English mathematician and inventor, is credited with "
    "designing the first automatic mechanical computer, which eventually led "
    "to more complex designs."
)
question = "Who is credited with inventing the first mechanical computer?"

# Step 3: Get prediction from the model
output_default = qa_pipeline(question=question, context=context)

print("\n--- Task 1: Default Model Output ---")
print(output_default)
# Expected: answer close to 'Charles Babbage', score > 0.65, valid start/end

# -----------------------------------------------------------
# Task 2: Use a Custom Pretrained Model (roberta-base-squad2)
# -----------------------------------------------------------

# Step 1: Initialize QA pipeline with custom model
custom_qa = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Step 2: Ask the same question with the same context
output_custom = custom_qa(question=question, context=context)

print("\n--- Task 2: Custom Model Output ---")
print(output_custom)
# Expected: answer = 'Charles Babbage', score > 0.70, valid start/end

# -----------------------------------------------------------
# Task 3: Test on Custom Example
# -----------------------------------------------------------

# Step 1: Define a new custom context
my_context = (
    "Sai Sravan Chintala is a data science enthusiast living in the United States. "
    "He is passionate about working with NLP and machine learning models."
)

# Step 2: Ask two questions based on the new context
my_question1 = "Where does Sai Sravan live?"
my_question2 = "What is Sai Sravan passionate about?"

# Step 3: Get predictions for each question
answer1 = custom_qa(question=my_question1, context=my_context)
answer2 = custom_qa(question=my_question2, context=my_context)

print("\n--- Task 3: Custom Context Question 1 ---")
print(answer1)
# Expected: answer = 'United States', score > 0.70

print("\n--- Task 3: Custom Context Question 2 ---")
print(answer2)
# Expected: answer = 'working with NLP and machine learning models', score > 0.70

"""Question2:
1.	Digit-Class Controlled Image Generation with Conditional GAN

Objective: Implement a Conditional GAN that generates MNIST digits based on a given class label (0–9). The goal is to understand how conditioning GANs on labels affects generation and how class control is added.

Task Description
1.	Modify a basic GAN to accept a digit label as input.
2.	Concatenate the label embedding with both:
*   the noise vector (input to Generator),
*   the image input (to the Discriminator).

3.	Train the cGAN on MNIST and generate digits conditioned on specific labels (e.g., generate only 3s or 7s).
4.	Visualize generated digits label by label (e.g., one row per digit class).

Expected Output

•	A row of 10 generated digits, each conditioned on labels 0 through 9.

•	Generator should learn to control output based on class.

•	Loss curves may still fluctuate, but quality and label accuracy improves over time.

"""

# Conditional GAN on MNIST
# Student: Sai Sravan Chintala
# Student ID: 700773836

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# ---------------------------------------------
# 1. Hyperparameters
# ---------------------------------------------
batch_size = 128
z_dim = 100
embedding_dim = 10
epochs = 50
lr = 0.0002
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ---------------------------------------------
# 2. Data Loader - MNIST
# ---------------------------------------------
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize([0.5], [0.5])
])

mnist = datasets.MNIST(root="./data", train=True, download=True, transform=transform)
data_loader = DataLoader(mnist, batch_size=batch_size, shuffle=True)

# ---------------------------------------------
# 3. Generator
# ---------------------------------------------
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.label_emb = nn.Embedding(10, embedding_dim)
        self.model = nn.Sequential(
            nn.Linear(z_dim + embedding_dim, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 784),
            nn.Tanh()
        )

    def forward(self, noise, labels):
        label_input = self.label_emb(labels)
        x = torch.cat((noise, label_input), dim=1)
        out = self.model(x)
        return out.view(-1, 1, 28, 28)

# ---------------------------------------------
# 4. Discriminator
# ---------------------------------------------
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.label_emb = nn.Embedding(10, embedding_dim)
        self.model = nn.Sequential(
            nn.Linear(784 + embedding_dim, 512),
            nn.LeakyReLU(0.2),
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )

    def forward(self, img, labels):
        img_flat = img.view(img.size(0), -1)
        label_input = self.label_emb(labels)
        x = torch.cat((img_flat, label_input), dim=1)
        return self.model(x)

# ---------------------------------------------
# 5. Training Setup
# ---------------------------------------------
generator = Generator().to(device)
discriminator = Discriminator().to(device)
criterion = nn.BCELoss()

g_optimizer = optim.Adam(generator.parameters(), lr=lr)
d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)

# ---------------------------------------------
# 6. Training Loop
# ---------------------------------------------
for epoch in range(epochs):
    for real_imgs, labels in data_loader:
        batch_size = real_imgs.size(0)
        real_imgs, labels = real_imgs.to(device), labels.to(device)

        # Labels
        valid = torch.ones(batch_size, 1).to(device)
        fake = torch.zeros(batch_size, 1).to(device)

        # ---------------------
        #  Train Discriminator
        # ---------------------
        z = torch.randn(batch_size, z_dim).to(device)
        gen_labels = torch.randint(0, 10, (batch_size,)).to(device)
        fake_imgs = generator(z, gen_labels)

        real_loss = criterion(discriminator(real_imgs, labels), valid)
        fake_loss = criterion(discriminator(fake_imgs.detach(), gen_labels), fake)
        d_loss = (real_loss + fake_loss) / 2

        d_optimizer.zero_grad()
        d_loss.backward()
        d_optimizer.step()

        # -----------------
        #  Train Generator
        # -----------------
        z = torch.randn(batch_size, z_dim).to(device)
        gen_labels = torch.randint(0, 10, (batch_size,)).to(device)
        fake_imgs = generator(z, gen_labels)
        g_loss = criterion(discriminator(fake_imgs, gen_labels), valid)

        g_optimizer.zero_grad()
        g_loss.backward()
        g_optimizer.step()

    print(f"[Epoch {epoch+1}/{epochs}] D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}")

# ---------------------------------------------
# 7. Visualization
# ---------------------------------------------
def generate_by_label():
    generator.eval()
    z = torch.randn(10, z_dim).to(device)
    labels = torch.arange(0, 10).to(device)
    gen_imgs = generator(z, labels).cpu().detach()

    fig, axes = plt.subplots(1, 10, figsize=(15, 2))
    for i in range(10):
        axes[i].imshow(gen_imgs[i].squeeze(), cmap='gray')
        axes[i].axis('off')
        axes[i].set_title(f"Label: {i}")
    plt.tight_layout()
    plt.savefig("generated_digits_by_label.png")
    plt.show()

generate_by_label()

"""Short Answer

•	How does a Conditional GAN differ from a vanilla GAN?

→ Include at least one real-world application where conditioning is important.

Answer: A Conditional GAN (cGAN) differs from a vanilla GAN by allowing both the generator and discriminator to receive additional input—a condition, such as a class label or image.

In a vanilla GAN, the generator creates data from random noise without context.

In a cGAN, the generator learns to create data based on a specific condition, and the discriminator uses that condition to evaluate authenticity.

Real-world application:
Image-to-image translation, like turning sketches into realistic photos, where the condition is the input sketch.

Text-to-image generation, where the condition is a textual description (e.g., “a red car on a bridge”).

•	What does the discriminator learn in an image-to-image GAN?

→ Why is pairing important in this context?

Answer: In an image-to-image GAN, the discriminator learns whether the generated image is a realistic and correct transformation of the input image—not just realism, but also relevance to the source.

Pairing ensures that the model learns structured relationships between inputs and outputs (e.g., turning a black-and-white image into the correct color version). Without paired data, the model can't learn how one image translates into another accurately.
"""